{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def DataGenerator(data_path, label_path, batch_size=64,maxlen=1000):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "\n",
    "    返回:\n",
    "        一个generator，x: 获取的批次图片 y: 获取的图片对应的标签\n",
    "    \"\"\"\n",
    "\n",
    "    fd = open(data_path)\n",
    "    fl = open(label_path)\n",
    "    datas = fd.readlines()\n",
    "    datas_tag = 0\n",
    "    labels = fl.readlines()\n",
    "    # print(len(datas), len(labels))\n",
    "    iter_num = int(len(labels) / batch_size)\n",
    "    print(iter_num)\n",
    "    i = 0\n",
    "    while iter_num:\n",
    "        irLine = []  #每行ir向量\n",
    "        irList = []  #ir切片向量列表\n",
    "        labelList = []  #label列表\n",
    "        vulList = []  #漏洞列表\n",
    "        matrixList = []\n",
    "        label_line = labels[i:i + batch_size]\n",
    "\n",
    "        for line in label_line:\n",
    "            line = line.strip()\n",
    "            a = line.split()\n",
    "            a = list(map(float, a))\n",
    "            if a[0] != 0:\n",
    "                vulList.append(a)\n",
    "                labelList.append(1)\n",
    "            else:\n",
    "                vulList.append(0)\n",
    "                labelList.append(0)\n",
    "\n",
    "        for vp in range(len(vulList)):\n",
    "            #先求漏洞行号标注在一个一维向量上\n",
    "            if not vulList[vp]:\n",
    "                attentionLine = [1] * maxlen\n",
    "            else:\n",
    "                attentionLine = [0] * maxlen\n",
    "                for vul in vulList[vp]:\n",
    "                    if int(vul) > maxlen:\n",
    "                        continue\n",
    "                    attentionLine[int(vul) - 1] = 1\n",
    "            #再将其转化为矩阵\n",
    "            attentionmatrix = np.diag(attentionLine)\n",
    "            matrixList.append(attentionmatrix)\n",
    "\n",
    "        while len(irList) < batch_size:\n",
    "            line = datas[datas_tag]\n",
    "            datas_tag += 1\n",
    "            #逐行遍历：行内字段按'\\t'分隔符分隔，转换为列表\n",
    "            line = line.strip()\n",
    "            a = line.split('\\t')\n",
    "            if '#' not in a[0]:\n",
    "                a = list(map(float, a))\n",
    "                irLine.append(a)\n",
    "            else:\n",
    "                x = [0 for t in range(300)]\n",
    "                while len(irLine) < maxlen:\n",
    "                    irLine.append(x)\n",
    "                irList.append(irLine)\n",
    "                irLine = []\n",
    "                continue\n",
    "\n",
    "\n",
    "        dataSet = np.array(irList)\n",
    "        labelSet = np.array(labelList)\n",
    "        matrixSet= np.array(matrixList)\n",
    "        # print(i)\n",
    "        # print(dataSet.ndim)\n",
    "        if dataSet.ndim!=3:\n",
    "            i+=batch_size\n",
    "            iter_num -= 1\n",
    "            if iter_num == 0:\n",
    "                iter_num = int(len(labels) / batch_size)\n",
    "                datas_tag=0\n",
    "                i = 0\n",
    "            continue\n",
    "        yield [dataSet,matrixSet], labelSet\n",
    "        i+=batch_size\n",
    "\n",
    "        iter_num -= 1\n",
    "        if iter_num == 0:\n",
    "            iter_num = int(len(labels) / batch_size)\n",
    "            datas_tag=0\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataGenerator(data_path, label_path, batch_size=64,maxlen=1000):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "\n",
    "    返回:\n",
    "        一个generator，x: 获取的批次图片 y: 获取的图片对应的标签\n",
    "    \"\"\"\n",
    "\n",
    "    fd = open(data_path)\n",
    "    fl = open(label_path)\n",
    "    datas = fd.readlines()\n",
    "    datas_tag = 0\n",
    "    labels = fl.readlines()\n",
    "    # print(len(datas), len(labels))\n",
    "    iter_num = int(len(labels) / batch_size)\n",
    "    print(iter_num)\n",
    "    i = 0\n",
    "    while iter_num:\n",
    "        irLine = []  #每行ir向量\n",
    "        irList = []  #ir切片向量列表\n",
    "        labelList = []  #label列表\n",
    "        vulList = []  #漏洞列表\n",
    "        matrixList = []\n",
    "        label_line = labels[i:i + batch_size]\n",
    "\n",
    "        for line in label_line:\n",
    "            line = line.strip()\n",
    "            a = line.split()\n",
    "            a = list(map(float, a))\n",
    "            if a[0] != 0:\n",
    "                vulList.append(a)\n",
    "                labelList.append(1)\n",
    "            else:\n",
    "                vulList.append(0)\n",
    "                labelList.append(0)\n",
    "\n",
    "        for vp in range(len(vulList)):\n",
    "            #先求漏洞行号标注在一个一维向量上\n",
    "            if not vulList[vp]:\n",
    "                attentionLine = [1] * maxlen\n",
    "            else:\n",
    "                attentionLine = [0] * maxlen\n",
    "                for vul in vulList[vp]:\n",
    "                    if int(vul) > maxlen:\n",
    "                        continue\n",
    "                    attentionLine[int(vul) - 1] = 1\n",
    "            #再将其转化为矩阵\n",
    "            attentionmatrix = np.diag(attentionLine)\n",
    "            matrixList.append(attentionmatrix)\n",
    "\n",
    "        while len(irList) < batch_size:\n",
    "            line = datas[datas_tag]\n",
    "            datas_tag += 1\n",
    "            #逐行遍历：行内字段按'\\t'分隔符分隔，转换为列表\n",
    "            line = line.strip()\n",
    "            a = line.split('\\t')\n",
    "            if '#' not in a[0]:\n",
    "                a = list(map(float, a))\n",
    "                irLine.append(a)\n",
    "            else:\n",
    "                x = [0 for t in range(300)]\n",
    "                while len(irLine) < maxlen:\n",
    "                    irLine.append(x)\n",
    "                irList.append(irLine)\n",
    "                irLine = []\n",
    "                continue\n",
    "\n",
    "\n",
    "        dataSet = np.array(irList)\n",
    "        labelSet = np.array(labelList)\n",
    "        matrixSet= np.array(matrixList)\n",
    "        # print(i)\n",
    "        # print(dataSet.ndim)\n",
    "        if dataSet.ndim!=3:\n",
    "            i+=batch_size\n",
    "            iter_num -= 1\n",
    "            if iter_num == 0:\n",
    "                iter_num = int(len(labels) / batch_size)\n",
    "                datas_tag=0\n",
    "                i = 0\n",
    "            continue\n",
    "        yield [dataSet, matrixSet], labelSet, vulList\n",
    "        i+=batch_size\n",
    "\n",
    "        iter_num -= 1\n",
    "        if iter_num == 0:\n",
    "            iter_num = int(len(labels) / batch_size)\n",
    "            datas_tag=0\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_label_path='../data/train_labelJuliet.txt'\n",
    "test_label_path='../data/test_labelJuliet.txt'\n",
    "validation_label_path='../data/validation_labelJuliet.txt'\n",
    "\n",
    "train_IR_path='../data/train_IRJuliet.txt'\n",
    "test_IR_path='../data/test_IRJuliet.txt'\n",
    "validation_IR_path='../data/validation_IRJuliet.txt'\n",
    "\n",
    "label_path='../data/label_Juliet.txt'\n",
    "data_path='../data/data_Juliet.txt'\n",
    "\n",
    "for i, ([data, attMatrix],label) in enumerate(DataGenerator(data_path, label_path, batch_size=64)):\n",
    "    print(i, data.shape, label.shape,attMatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgru_1 (Bidirectional)          (None, 1000, 128)    140544      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 128)    0           bgru_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bgru_2 (Bidirectional)          (None, 1000, 128)    74496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000, 128)    0           bgru_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (TimeDistributed)        (None, 1000, 1)      129         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_Matrix (InputLayer)         [(None, 1000, 1000)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000, 1)      0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1000, 1000)   0           att_Matrix[0][0]                 \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 1000000)   0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 1)         0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (GlobalAveragePooling (None, 1)            0           max_pooling1d[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 215,169\n",
      "Trainable params: 215,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras import Input, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import GRU, Bidirectional, Dropout, TimeDistributed, Dense, Activation, GlobalAveragePooling1D,\\\n",
    "    Reshape, MaxPooling1D, Multiply\n",
    "\n",
    "def build_model(maxlen, dropout, units):\n",
    "    inputs = Input(shape=(maxlen, 300))\n",
    "\n",
    "    bgru_1 = Bidirectional(GRU(units=units, activation='tanh', recurrent_activation='sigmoid', return_sequences=True),\n",
    "                           name='bgru_1')(inputs)\n",
    "    dropout_1 = Dropout(rate=dropout, name='dropout_1')(bgru_1)\n",
    "    bgru_2 = Bidirectional(GRU(units=units, activation='tanh', recurrent_activation='sigmoid', return_sequences=True),\n",
    "                           name='bgru_2')(dropout_1)\n",
    "    dropout_2 = Dropout(rate=dropout, name='dropout_2')(bgru_2)\n",
    "\n",
    "    dense_1 = TimeDistributed(Dense(1), name='dense1')(dropout_2)\n",
    "    activation_1 = Activation('sigmoid', name='activation_1')(dense_1)\n",
    "\n",
    "    att_Matrix_1 = Input(shape=(maxlen, maxlen), name='att_Matrix')\n",
    "    multiply_1 = Multiply(name='multiply_1')([att_Matrix_1, activation_1])\n",
    "    reshape_1 = Reshape((1, maxlen ** 2))(multiply_1)\n",
    "\n",
    "    k_max_1 = MaxPooling1D(pool_size=maxlen ** 2, data_format='channels_first')(reshape_1)\n",
    "    average_1 = GlobalAveragePooling1D(name='average_1')(k_max_1)\n",
    "\n",
    "    model = Model(inputs=[inputs, att_Matrix_1], outputs=average_1)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\",\n",
    "                           \"Precision\",\n",
    "                           \"Recall\",\n",
    "                           \"TruePositives\",\n",
    "                           \"TrueNegatives\",\n",
    "                           \"FalsePositives\",\n",
    "                           \"FalseNegatives\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(maxlen=1000,dropout=0.4,units=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train_IRJuliet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 36>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     34\u001B[0m tdg\u001B[38;5;241m=\u001B[39mDataGenerator(train_IR_path, train_label_path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[0;32m     35\u001B[0m vdg\u001B[38;5;241m=\u001B[39mDataGenerator(validation_IR_path,validation_label_path,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43mtrain_model2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtdg\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvdg\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mtrain_model2\u001B[1;34m(model, TrainGenerator, ValidGenerator)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model2\u001B[39m(model,TrainGenerator,ValidGenerator):\n\u001B[0;32m      2\u001B[0m     callbacks \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      3\u001B[0m         tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[0;32m      4\u001B[0m             \u001B[38;5;66;03m# 模型路径\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m         )\n\u001B[0;32m     13\u001B[0m     ]\n\u001B[1;32m---> 16\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTrainGenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m              \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m227\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mValidGenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m              \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m              \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:1134\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1128\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cluster_coordinator \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mcoordinator\u001B[38;5;241m.\u001B[39mClusterCoordinator(\n\u001B[0;32m   1129\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy\u001B[38;5;241m.\u001B[39mscope(), \\\n\u001B[0;32m   1132\u001B[0m      training_utils\u001B[38;5;241m.\u001B[39mRespectCompiledTrainableState(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1133\u001B[0m   \u001B[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001B[39;00m\n\u001B[1;32m-> 1134\u001B[0m   data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1135\u001B[0m \u001B[43m      \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m      \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m      \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m      \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m      \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m      \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m      \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m      \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[43m      \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1147\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1148\u001B[0m \u001B[43m      \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1150\u001B[0m   \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[0;32m   1151\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001B[0m, in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1382\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1135\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution_value \u001B[38;5;241m=\u001B[39m steps_per_execution\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m   1137\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m select_data_adapter(x, y)\n\u001B[1;32m-> 1138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m \u001B[43madapter_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1148\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistribution_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1152\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:794\u001B[0m, in \u001B[0;36mGeneratorDataAdapter.__init__\u001B[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001B[0m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;28msuper\u001B[39m(GeneratorDataAdapter, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(x, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    792\u001B[0m \u001B[38;5;66;03m# Since we have to know the dtype of the python generator when we build the\u001B[39;00m\n\u001B[0;32m    793\u001B[0m \u001B[38;5;66;03m# dataset, we have to look at a batch to infer the structure.\u001B[39;00m\n\u001B[1;32m--> 794\u001B[0m peek, x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_peek_and_restore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    795\u001B[0m peek \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_standardize_batch(peek)\n\u001B[0;32m    796\u001B[0m peek \u001B[38;5;241m=\u001B[39m _process_tensorlike(peek)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:851\u001B[0m, in \u001B[0;36mGeneratorDataAdapter._peek_and_restore\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    850\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_peek_and_restore\u001B[39m(x):\n\u001B[1;32m--> 851\u001B[0m   peek \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    852\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m peek, itertools\u001B[38;5;241m.\u001B[39mchain([peek], x)\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mDataGenerator\u001B[1;34m(data_path, label_path, batch_size, maxlen)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mDataGenerator\u001B[39m(data_path, label_path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m,maxlen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m    参数：\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m    返回:\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m        一个generator，x: 获取的批次图片 y: 获取的图片对应的标签\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m     fd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     fl \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(label_path)\n\u001B[0;32m     16\u001B[0m     datas \u001B[38;5;241m=\u001B[39m fd\u001B[38;5;241m.\u001B[39mreadlines()\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/train_IRJuliet.txt'"
     ]
    }
   ],
   "source": [
    "def train_model2(model,TrainGenerator,ValidGenerator):\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            # 模型路径\n",
    "            filepath='./model/model_{epoch:02d}-{val_accuracy:.2f}.h5',\n",
    "            # 是否保存最佳\n",
    "            save_best_only=True,\n",
    "            # 监控指标\n",
    "            monitor='val_accuracy',\n",
    "            # 进度条类型\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "    model.fit(TrainGenerator,\n",
    "              steps_per_epoch=227,\n",
    "              epochs=10,\n",
    "              validation_data=ValidGenerator,\n",
    "              validation_steps=5,\n",
    "              validation_batch_size=64,\n",
    "              verbose=1,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "train_IR_path='../data/train_IRJuliet.txt'\n",
    "validation_IR_path='../data/validation_IRJuliet.txt'\n",
    "\n",
    "train_label_path='../data/train_labelJuliet.txt'\n",
    "validation_label_path='../data/validation_labelJuliet.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tdg=DataGenerator(train_IR_path, train_label_path, batch_size=64)\n",
    "vdg=DataGenerator(validation_IR_path,validation_label_path,batch_size=64)\n",
    "train_model2(model,tdg,vdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "15\n",
      "TP:15 FP:4 FN:2 TN:43\n",
      "TP_l:15 FP_l:4 FN_l:2 TN:43\n",
      "epochs: 1\n",
      "TP:27 FP:4 FN:4 TN:93\n",
      "TP_l:27 FP_l:4 FN_l:4 TN:93\n",
      "epochs: 2\n",
      "TP:40 FP:4 FN:8 TN:140\n",
      "TP_l:40 FP_l:4 FN_l:8 TN:140\n",
      "epochs: 3\n",
      "TP:57 FP:5 FN:12 TN:182\n",
      "TP_l:57 FP_l:5 FN_l:12 TN:182\n",
      "TP:57 FP:5 FN:12 TN:182\n",
      "\n",
      "FPR: 0.026737967914438502\n",
      "\n",
      "FNR: 0.17391304347826086\n",
      "\n",
      "accuracy: 0.93359375\n",
      "\n",
      "precision: 0.9193548387096774\n",
      "\n",
      "recall: 0.8260869565217391\n",
      "\n",
      "F1_score: 0.8702290076335878\n",
      "\n",
      "\n",
      "TP_l:57 FP_l:5 FN_l:12 TN:182\n",
      "\n",
      "FPR_location: 0.026737967914438502\n",
      "\n",
      "FNR_location: 0.17391304347826086\n",
      "\n",
      "accuracy_location: 0.93359375\n",
      "\n",
      "precision_location: 0.9193548387096774\n",
      "\n",
      "recall_location: 0.8260869565217391\n",
      "\n",
      "loU: 0.11617313027226238\n",
      "\n",
      "loU: 0.11617313027226238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from keras import Model, models\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "def get_predict_line(value_sequence, threshold_value=0.5):\n",
    "    value_sequence = list(value_sequence)\n",
    "    vs = len(value_sequence) - 1\n",
    "    while value_sequence[vs] == value_sequence[-1]:\n",
    "        vs -= 1\n",
    "    value_sequence = value_sequence[:vs + 2]\n",
    "\n",
    "    predict_line = []\n",
    "    for i in range(len(value_sequence)):\n",
    "        if value_sequence[i] > threshold_value:\n",
    "            predict_line.append(i)\n",
    "    return predict_line\n",
    "\n",
    "\n",
    "def test_model(model, datapath,labelpath,result_path):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    TP_l, TN_l, FP_l, FN_l = 0, 0, 0, 0\n",
    "    loU_list = []\n",
    "    partial_model = Model(inputs=model.layers[0].input, outputs=model.layers[7].output)\n",
    "    test_data=TestDataGenerator(datapath,labelpath,batch_size=64)\n",
    "    batch_size=64\n",
    "    iter_num=92\n",
    "    for i in range(4):\n",
    "        print(\"epochs: \"+str(i))\n",
    "        td=next(test_data)\n",
    "        output_test = partial_model([td[0][0]], training=False)\n",
    "        label=td[1]\n",
    "        vul_line=td[2]\n",
    "        # print(output_test.shape,label.shape,len(vul_line))\n",
    "        for j in range(batch_size):\n",
    "            predict_line = get_predict_line(output_test[j])\n",
    "            # print(output_test[i])\n",
    "            # print(predict_line)\n",
    "            # print(label)\n",
    "            if predict_line:\n",
    "                label_pred = 1\n",
    "            else:\n",
    "                label_pred = 0\n",
    "            # print(label_pred,label[j])\n",
    "            if label_pred == 0 and label[j] == 0:\n",
    "                TN += 1\n",
    "                TN_l += 1\n",
    "            if label_pred == 0 and label[j] == 1:\n",
    "                FN += 1\n",
    "                FN_l += 1\n",
    "            if label_pred == 1 and label[j] == 0:\n",
    "                FP += 1\n",
    "                FP_l += 1\n",
    "            if label_pred == 1 and label[j] == 1:\n",
    "                TP += 1\n",
    "                flag_l = False\n",
    "                for pred in predict_line:\n",
    "                    if pred in vul_line[j]:\n",
    "                        flag_l = True\n",
    "                        break\n",
    "                if flag_l:\n",
    "                    TP_l += 1\n",
    "                else:\n",
    "                    FN_l += 1\n",
    "                overlap_line = list(set(predict_line).intersection(set(vul_line[j])))\n",
    "                union_line = list(set(predict_line).union(set(vul_line[j])))\n",
    "                loU = len(overlap_line) / len(union_line)\n",
    "                loU_list.append(loU)\n",
    "        print('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN))\n",
    "        print('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l))\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    accuracy = (TP + TN) / (TP+TN+FP+FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    FPR_line = FP_l / (FP_l + TN_l)\n",
    "    FNR_line = FN_l / (TP_l + FN_l)\n",
    "    accuracy_line = (TP_l + TN_l) / (TP_l + FP_l + FN_l + TN_l)\n",
    "    precision_line = TP_l / (TP_l + FP_l)\n",
    "    recall_line = TP_l / (TP_l + FN_l)\n",
    "    F1_score_line=(2 * precision_line * recall_line) / (precision_line + recall_line)\n",
    "\n",
    "    loU = mean(loU_list)\n",
    "    # print('test_samples_num: ' + str(len(data)) + '\\n')\n",
    "    print('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN) + '\\n')\n",
    "    print('FPR: ' + str(FPR) + '\\n')\n",
    "    print('FNR: ' + str(FNR) + '\\n')\n",
    "    print('accuracy: ' + str(accuracy) + '\\n')\n",
    "    print('precision: ' + str(precision) + '\\n')\n",
    "    print('recall: ' + str(recall) + '\\n')\n",
    "    print('F1_score: ' + str(F1_score) + '\\n\\n')\n",
    "\n",
    "    print('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l) + '\\n')\n",
    "    print('FPR_location: ' + str(FPR_line) + '\\n')\n",
    "    print('FNR_location: ' + str(FNR_line) + '\\n')\n",
    "    print('accuracy_location: ' + str(accuracy_line) + '\\n')\n",
    "    print('precision_location: ' + str(precision_line) + '\\n')\n",
    "    print('recall_location: ' + str(recall_line) + '\\n')\n",
    "    print('F1_score_location: ' + str(F1_score_line) + '\\n\\n')\n",
    "\n",
    "    print('loU: ' + str(loU) + '\\n')\n",
    "\n",
    "\n",
    "    with open(result_path, 'a') as fwrite:\n",
    "        # fwrite.write('test_samples_num: ' + str(len(data)) + '\\n')\n",
    "        fwrite.write('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN) + '\\n')\n",
    "        fwrite.write('FPR: ' + str(FPR) + '\\n')\n",
    "        fwrite.write('FNR: ' + str(FNR) + '\\n')\n",
    "        fwrite.write('accuracy: ' + str(accuracy) + '\\n')\n",
    "        fwrite.write('precision: ' + str(precision) + '\\n')\n",
    "        fwrite.write('recall: ' + str(recall) + '\\n')\n",
    "        fwrite.write('F1_score: ' + str(F1_score) + '\\n\\n')\n",
    "\n",
    "        fwrite.write('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l) + '\\n')\n",
    "        fwrite.write('FPR_location: ' + str(FPR_line) + '\\n')\n",
    "        fwrite.write('FNR_location: ' + str(FNR_line) + '\\n')\n",
    "        fwrite.write('accuracy_location: ' + str(accuracy_line) + '\\n')\n",
    "        fwrite.write('precision_location: ' + str(precision_line) + '\\n')\n",
    "        fwrite.write('recall_location: ' + str(recall_line) + '\\n')\n",
    "        fwrite.write('F1_score_location: ' + str(F1_score_line) + '\\n\\n')\n",
    "\n",
    "        fwrite.write('loU: ' + str(loU) + '\\n')\n",
    "\n",
    "\n",
    "modelPath = './model/model_10-0.97.h5'\n",
    "resultPath = './result/result_model_10_0.97.txt'\n",
    "\n",
    "# test_IR_path = '../data/test_IRJuliet.txt'\n",
    "# test_label_path = '../data/test_labelJuliet.txt'\n",
    "test_IR_path = '../data/data_Juliet2.txt'\n",
    "test_label_path ='../data/label_Juliet2.txt'\n",
    "\n",
    "model = models.load_model(modelPath)\n",
    "test_model(model, test_IR_path, test_label_path, resultPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## cs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(64, 1000, 1) (64,) 64\n",
      "[0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_data = TestDataGenerator(test_IR_path,test_label_path,batch_size=64)\n",
    "td=next(test_data)\n",
    "partial_model = Model(inputs=model.layers[0].input, outputs=model.layers[7].output)\n",
    "output_test = partial_model([td[0][0]], training=False)\n",
    "label=td[1]\n",
    "vul_line=td[2]\n",
    "print(output_test.shape,label.shape,len(vul_line))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n",
      "[26.0, 27.0, 28.0]\n",
      "[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(vul_line[4])\n",
    "# print(output_test[4][0:50])\n",
    "predict_line = get_predict_line(output_test[4])\n",
    "print(predict_line)\n",
    "if predict_line:\n",
    "    label_pred = 1\n",
    "else:\n",
    "    label_pred = 0\n",
    "print(label_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m td \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[43mtest_data\u001B[49m)\n\u001B[0;32m      2\u001B[0m output\u001B[38;5;241m=\u001B[39mmodel([td[\u001B[38;5;241m0\u001B[39m]],training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m label\u001B[38;5;241m=\u001B[39mtd[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "td = next(test_data)\n",
    "output=model([td[0]],training=False)\n",
    "label=td[1]\n",
    "print(label)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "  9/100 [=>............................] - ETA: 4:04 - loss: 0.1923 - accuracy: 0.9306 - precision: 0.9638 - recall: 0.7917 - true_positives: 133.0000 - true_negatives: 403.0000 - false_positives: 5.0000 - false_negatives: 35.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m test_data2\u001B[38;5;241m=\u001B[39mDataGenerator(test_IR_path,test_label_path,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data2\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:1501\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1500\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[1;32m-> 1501\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1503\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    882\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 885\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    887\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    888\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    921\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    922\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 924\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateful_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[0;32m    926\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    927\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3036\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   3037\u001B[0m   (graph_function,\n\u001B[0;32m   3038\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 3039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3040\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1959\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1961\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1962\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1963\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1964\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1965\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1966\u001B[0m     args,\n\u001B[0;32m   1967\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1968\u001B[0m     executing_eagerly)\n\u001B[0;32m   1969\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    590\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 591\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    597\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    598\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    599\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    600\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    603\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    604\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 59\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     62\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_data2=DataGenerator(test_IR_path,test_label_path,batch_size=64)\n",
    "model.evaluate(test_data2,batch_size=64,steps=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dab8e41921a9804032d7b1bd163ac623c44de801f9b5a9764714bd0c64231e68"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}