{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cutting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24235/24235 [00:00<00:00, 1349937.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num: 600\n",
      "test_num: 200\n",
      "validation_num: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293765750/293765750 [00:05<00:00, 55946300.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras import Input, Model, models\n",
    "import tensorflow as tf\n",
    "from keras.layers import GRU, Bidirectional, Dropout, TimeDistributed, Dense, Activation, GlobalAveragePooling1D,\n",
    "    Reshape, MaxPooling1D, Multiply\n",
    "\n",
    "\n",
    "def DataCutting(irpath, labelpath, datafile):\n",
    "    \"\"\"数据切割\n",
    "\n",
    "    :param irpath: 待切割ir数据地址\n",
    "    :param labelpath: 待切割标签地址\n",
    "    :param datafile: 切割后数据存放文件夹\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    #切割完后存放标签的地址\n",
    "    train_label_path = datafile + '/train_label.txt'\n",
    "    test_label_path = datafile + '/test_label.txt'\n",
    "    validation_label_path = datafile + '/validation_label.txt'\n",
    "    #切割完后存放IR向量地址\n",
    "    train_IR_path = datafile + '/train_IR.txt'\n",
    "    test_IR_path = datafile + '/test_IR.txt'\n",
    "    validation_IR_path = datafile + '/validation_IR.txt'\n",
    "\n",
    "    #切割方式为每十行都把数据划分为6:2:2，分别为训练集、验证集、测试集\n",
    "    #pbar用于进度条显示\n",
    "    with tqdm(total=os.path.getsize(labelpath)) as pbar:\n",
    "        with open(labelpath, 'r') as f1:\n",
    "            train_label = open(train_label_path, 'a', encoding='UTF-8')\n",
    "            test_label = open(test_label_path, 'a', encoding='UTF-8')\n",
    "            validation_label = open(validation_label_path, 'a', encoding='UTF-8')\n",
    "            t = 0\n",
    "            train_num = 0\n",
    "            test_num = 0\n",
    "            validation_num = 0\n",
    "            for line in f1:\n",
    "                t += 1\n",
    "                pbar.update(len(line))\n",
    "                if t <= 6:\n",
    "                    train_label.write(line)\n",
    "                    train_num += 1\n",
    "                elif t <= 8:\n",
    "                    test_label.write(line)\n",
    "                    test_num += 1\n",
    "                elif t <= 10:\n",
    "                    validation_label.write(line)\n",
    "                    validation_num += 1\n",
    "                    if t == 10:\n",
    "                        t = 0\n",
    "\n",
    "            train_label.close()\n",
    "            test_label.close()\n",
    "            validation_label.close()\n",
    "            print(\"train_num: \" + str(train_num))\n",
    "            print(\"test_num: \" + str(test_num))\n",
    "            print(\"validation_num: \" + str(validation_num))\n",
    "\n",
    "    with tqdm(total=os.path.getsize(irpath)) as pbar:\n",
    "        with open(irpath, 'r') as f1:\n",
    "            train_IR = open(train_IR_path, 'a', encoding='UTF-8')\n",
    "            test_IR = open(test_IR_path, 'a', encoding='UTF-8')\n",
    "            validation_IR = open(validation_IR_path, 'a', encoding='UTF-8')\n",
    "            t = 0\n",
    "            k = 0\n",
    "            for line in f1:\n",
    "                pbar.update(len(line))\n",
    "                if t < 6:\n",
    "                    train_IR.write(line)\n",
    "                elif t < 8:\n",
    "                    test_IR.write(line)\n",
    "                elif t < 10:\n",
    "                    validation_IR.write(line)\n",
    "                if line.find('#') != -1:\n",
    "                    t += 1\n",
    "                    k += 1\n",
    "                    if t == 10:\n",
    "                        t = 0\n",
    "\n",
    "            print(\"data_sum: \" + str(k))\n",
    "\n",
    "            train_IR.close()\n",
    "            test_IR.close()\n",
    "            validation_IR.close()\n",
    "\n",
    "\n",
    "#原始数据地址\n",
    "label_path = '../data/label_Juliet2.txt'\n",
    "IR_path = '../data/data_Juliet2.txt'\n",
    "#切割后数据存放文件夹\n",
    "data_file = '../data/Juliet'\n",
    "DataCutting(IR_path, label_path, data_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TrainDataGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def TrainDataGenerator(data_path, label_path, batch_size=64, maxlen=1000):\n",
    "    \"\"\"训练数据集生成器\n",
    "\n",
    "    :param data_path: 数据集路径\n",
    "    :param label_path: 行号标签路径\n",
    "    :param batch_size: 批次大小\n",
    "    :param maxlen: 时间步维度，即最大保留多少行号\n",
    "    :return: 一个generator,形式为([dataSet,matrixSet], labelSet)\n",
    "        dataSet:IR向量数据集\n",
    "        matrixSet:注意力矩阵\n",
    "        labelSet:行号标签\n",
    "    \"\"\"\n",
    "\n",
    "    fd = open(data_path)\n",
    "    fl = open(label_path)\n",
    "    datas = fd.readlines()\n",
    "    datas_tag = 0\n",
    "    labels = fl.readlines()\n",
    "    # print(len(datas), len(labels))\n",
    "    iter_num = int(len(labels) / batch_size)\n",
    "    print(iter_num)\n",
    "    i = 0\n",
    "    while iter_num:\n",
    "        irLine = []  #每行ir向量\n",
    "        irList = []  #ir切片向量列表\n",
    "        labelList = []  #label列表\n",
    "        vulList = []  #漏洞列表\n",
    "        matrixList = []\n",
    "        label_line = labels[i:i + batch_size]\n",
    "\n",
    "        for line in label_line:\n",
    "            line = line.strip()\n",
    "            a = line.split()\n",
    "            a = list(map(float, a))\n",
    "            if a[0] != 0:\n",
    "                vulList.append(a)\n",
    "                labelList.append(1)\n",
    "            else:\n",
    "                vulList.append(0)\n",
    "                labelList.append(0)\n",
    "\n",
    "        for vp in range(len(vulList)):\n",
    "            #先求漏洞行号标注在一个一维向量上\n",
    "            if not vulList[vp]:\n",
    "                attentionLine = [1] * maxlen\n",
    "            else:\n",
    "                attentionLine = [0] * maxlen\n",
    "                for vul in vulList[vp]:\n",
    "                    if int(vul) > maxlen:\n",
    "                        continue\n",
    "                    attentionLine[int(vul) - 1] = 1\n",
    "            #再将其转化为矩阵\n",
    "            attentionmatrix = np.diag(attentionLine)\n",
    "            matrixList.append(attentionmatrix)\n",
    "\n",
    "        while len(irList) < batch_size:\n",
    "            line = datas[datas_tag]\n",
    "            datas_tag += 1\n",
    "            #逐行遍历：行内字段按'\\t'分隔符分隔，转换为列表\n",
    "            line = line.strip()\n",
    "            a = line.split('\\t')\n",
    "            if '#' not in a[0]:\n",
    "                a = list(map(float, a))\n",
    "                irLine.append(a)\n",
    "            else:\n",
    "                x = [0 for t in range(300)]\n",
    "                while len(irLine) < maxlen:\n",
    "                    irLine.append(x)\n",
    "                irList.append(irLine)\n",
    "                irLine = []\n",
    "                continue\n",
    "\n",
    "        dataSet = np.array(irList)\n",
    "        labelSet = np.array(labelList)\n",
    "        matrixSet = np.array(matrixList)\n",
    "        # print(i)\n",
    "        # print(dataSet.ndim)\n",
    "        if dataSet.ndim != 3:\n",
    "            i += batch_size\n",
    "            iter_num -= 1\n",
    "            if iter_num == 0:\n",
    "                iter_num = int(len(labels) / batch_size)\n",
    "                datas_tag = 0\n",
    "                i = 0\n",
    "            continue\n",
    "        yield [dataSet, matrixSet], labelSet\n",
    "        i += batch_size\n",
    "\n",
    "        iter_num -= 1\n",
    "        if iter_num == 0:\n",
    "            iter_num = int(len(labels) / batch_size)\n",
    "            datas_tag = 0\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TestDataGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataGenerator(data_path, label_path, batch_size=64, maxlen=1000):\n",
    "    \"\"\"测试数据集生成器\n",
    "\n",
    "    :param data_path: 数据集路径\n",
    "    :param label_path: 行号标签路径\n",
    "    :param batch_size: 批次大小\n",
    "    :param maxlen: 时间步维度，即最大保留多少行号\n",
    "    :return: 一个generator,形式为([dataSet,matrixSet], labelSet,vulList)\n",
    "        dataSet:IR向量数据集\n",
    "        matrixSet:注意力矩阵\n",
    "        labelSet:行号01标签\n",
    "        vulList:行号标签\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fd = open(data_path)\n",
    "    fl = open(label_path)\n",
    "    datas = fd.readlines()\n",
    "    labels = fl.readlines()\n",
    "    # print(len(datas), len(labels))\n",
    "    iter_num = int(len(labels) / batch_size)\n",
    "    print(iter_num)\n",
    "    datas_tag = 0\n",
    "    i = 0\n",
    "    while iter_num:\n",
    "        irLine = []  #每行ir向量\n",
    "        irList = []  #ir切片向量列表\n",
    "        labelList = []  #label列表\n",
    "        vulList = []  #漏洞列表\n",
    "        matrixList = []\n",
    "        label_line = labels[i:i + batch_size]\n",
    "\n",
    "        for line in label_line:\n",
    "            line = line.strip()\n",
    "            a = line.split()\n",
    "            a = list(map(float, a))\n",
    "            if a[0] != 0:\n",
    "                vulList.append(a)\n",
    "                labelList.append(1)\n",
    "            else:\n",
    "                vulList.append(0)\n",
    "                labelList.append(0)\n",
    "\n",
    "        for vp in range(len(vulList)):\n",
    "            #先求漏洞行号标注在一个一维向量上\n",
    "            if not vulList[vp]:\n",
    "                attentionLine = [1] * maxlen\n",
    "            else:\n",
    "                attentionLine = [0] * maxlen\n",
    "                for vul in vulList[vp]:\n",
    "                    if int(vul) > maxlen:\n",
    "                        continue\n",
    "                    attentionLine[int(vul) - 1] = 1\n",
    "            #再将其转化为矩阵\n",
    "            attentionmatrix = np.diag(attentionLine)\n",
    "            matrixList.append(attentionmatrix)\n",
    "\n",
    "        while len(irList) < batch_size:\n",
    "            line = datas[datas_tag]\n",
    "            datas_tag += 1\n",
    "            #逐行遍历：行内字段按'\\t'分隔符分隔，转换为列表\n",
    "            line = line.strip()\n",
    "            a = line.split('\\t')\n",
    "            if '#' not in a[0]:\n",
    "                a = list(map(float, a))\n",
    "                irLine.append(a)\n",
    "            else:\n",
    "                x = [0 for t in range(300)]\n",
    "                while len(irLine) < maxlen:\n",
    "                    irLine.append(x)\n",
    "                irList.append(irLine)\n",
    "                irLine = []\n",
    "                continue\n",
    "\n",
    "        dataSet = np.array(irList)\n",
    "        labelSet = np.array(labelList)\n",
    "        matrixSet = np.array(matrixList)\n",
    "        # print(i)\n",
    "        # print(dataSet.ndim)\n",
    "        if dataSet.ndim != 3:\n",
    "            i += batch_size\n",
    "            iter_num -= 1\n",
    "            if iter_num == 0:\n",
    "                iter_num = int(len(labels) / batch_size)\n",
    "                datas_tag = 0\n",
    "                i = 0\n",
    "            continue\n",
    "        yield [dataSet, matrixSet], labelSet, vulList\n",
    "        i += batch_size\n",
    "\n",
    "        iter_num -= 1\n",
    "        if iter_num == 0:\n",
    "            iter_num = int(len(labels) / batch_size)\n",
    "            datas_tag = 0\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0 (64, 1000, 300) (64,) (64, 1000, 1000)\n",
      "1 (64, 1000, 300) (64,) (64, 1000, 1000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m label_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/label_Juliet2.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     10\u001B[0m data_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/data_Juliet2.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, ([data, attMatrix],label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(TrainDataGenerator(train_IR_path, train_label_path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i, data\u001B[38;5;241m.\u001B[39mshape, label\u001B[38;5;241m.\u001B[39mshape,attMatrix\u001B[38;5;241m.\u001B[39mshape)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mTrainDataGenerator\u001B[1;34m(data_path, label_path, batch_size, maxlen)\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     76\u001B[0m dataSet \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(irList)\n\u001B[1;32m---> 77\u001B[0m labelSet \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39marray(labelList)\n\u001B[0;32m     78\u001B[0m matrixSet\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(matrixList)\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# print(i)\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# print(dataSet.ndim)\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_IR_path = '../data/Juliet/train_IR.txt'\n",
    "test_IR_path = '../data/Juliet/test_IR.txt'\n",
    "validation_IR_path = '../data/Juliet/validation_IR.txt'\n",
    "\n",
    "train_label_path = '../data/Juliet/train_label.txt'\n",
    "test_label_path = '../data/Juliet/test_label.txt'\n",
    "validation_label_path = '../data/Juliet/validation_label.txt'\n",
    "\n",
    "label_path = '../data/label_Juliet2.txt'\n",
    "data_path = '../data/data_Juliet2.txt'\n",
    "\n",
    "for i, ([data, attMatrix], label) in enumerate(TrainDataGenerator(train_IR_path, train_label_path, batch_size=64)):\n",
    "    print(i, data.shape, label.shape, attMatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1000, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgru_1 (Bidirectional)          (None, 1000, 128)    140544      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 128)    0           bgru_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bgru_2 (Bidirectional)          (None, 1000, 128)    74496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000, 128)    0           bgru_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (TimeDistributed)        (None, 1000, 1)      129         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_Matrix (InputLayer)         [(None, 1000, 1000)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000, 1)      0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1000, 1000)   0           att_Matrix[0][0]                 \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1000000)   0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 1)         0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (GlobalAveragePooling (None, 1)            0           max_pooling1d_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 215,169\n",
      "Trainable params: 215,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(maxlen, dropout, units):\n",
    "    \"\"\"\n",
    "\n",
    "    :param maxlen:最大时间步(最大行号)\n",
    "    :param dropout:随机让神经元停止工作的概率\n",
    "    :param units:GRU神经元数量\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(maxlen, 300))\n",
    "    bgru_1 = Bidirectional(GRU(units=units,\n",
    "                               activation='tanh',\n",
    "                               recurrent_activation='sigmoid',\n",
    "                               return_sequences=True),\n",
    "                           name='bgru_1')(inputs)\n",
    "    dropout_1 = Dropout(rate=dropout, name='dropout_1')(bgru_1)\n",
    "    bgru_2 = Bidirectional(GRU(units=units,\n",
    "                               activation='tanh',\n",
    "                               recurrent_activation='sigmoid',\n",
    "                               return_sequences=True),\n",
    "                           name='bgru_2')(dropout_1)\n",
    "    dropout_2 = Dropout(rate=dropout, name='dropout_2')(bgru_2)\n",
    "\n",
    "    dense_1 = TimeDistributed(Dense(1), name='dense1')(dropout_2)\n",
    "    activation_1 = Activation('sigmoid', name='activation_1')(dense_1)\n",
    "\n",
    "    att_Matrix_1 = Input(shape=(maxlen, maxlen), name='att_Matrix')\n",
    "    multiply_1 = Multiply(name='multiply_1')([att_Matrix_1, activation_1])\n",
    "    reshape_1 = Reshape((1, maxlen ** 2))(multiply_1)\n",
    "\n",
    "    k_max_1 = MaxPooling1D(pool_size=maxlen ** 2, data_format='channels_first')(reshape_1)\n",
    "    average_1 = GlobalAveragePooling1D(name='average_1')(k_max_1)\n",
    "\n",
    "    model = Model(inputs=[inputs, att_Matrix_1], outputs=average_1)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\",\n",
    "                           \"Precision\",\n",
    "                           \"Recall\",\n",
    "                           \"TruePositives\",\n",
    "                           \"TrueNegatives\",\n",
    "                           \"FalsePositives\",\n",
    "                           \"FalseNegatives\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(maxlen=1000, dropout=0.4, units=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Epoch 1/10\n",
      "  2/227 [..............................] - ETA: 28:41 - loss: 0.7225 - accuracy: 0.4688 - precision: 0.2031 - recall: 0.4333 - true_positives: 13.0000 - true_negatives: 47.0000 - false_positives: 51.0000 - false_negatives: 17.0000    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 34>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     32\u001B[0m tdg\u001B[38;5;241m=\u001B[39mTrainDataGenerator(train_IR_path, train_label_path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[0;32m     33\u001B[0m vdg\u001B[38;5;241m=\u001B[39mTrainDataGenerator(validation_IR_path,validation_label_path,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m---> 34\u001B[0m \u001B[43mtrain_model2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtdg\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvdg\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mtrain_model2\u001B[1;34m(model, TrainGenerator, ValidGenerator)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model2\u001B[39m(model,TrainGenerator,ValidGenerator):\n\u001B[0;32m      2\u001B[0m     callbacks \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      3\u001B[0m         tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[0;32m      4\u001B[0m             \u001B[38;5;66;03m# 模型路径\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m         )\n\u001B[0;32m     13\u001B[0m     ]\n\u001B[1;32m---> 16\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTrainGenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m              \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m227\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m              \u001B[49m\u001B[38;5;66;43;03m#每批次总步数\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                        \u001B[49m\u001B[38;5;66;43;03m#批次数\u001B[39;49;00m\n\u001B[0;32m     19\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mValidGenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m#验证集\u001B[39;49;00m\n\u001B[0;32m     20\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m               \u001B[49m\u001B[38;5;66;43;03m#每次验证步数\u001B[39;49;00m\n\u001B[0;32m     21\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m         \u001B[49m\u001B[38;5;66;43;03m#验证集每批次数据大小\u001B[39;49;00m\n\u001B[0;32m     22\u001B[0m \u001B[43m              \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m              \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:1184\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1178\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1179\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1180\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1181\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1182\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1183\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1184\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1185\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1186\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    882\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 885\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    887\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    888\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    914\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    915\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    916\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 917\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    919\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    920\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    921\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3036\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   3037\u001B[0m   (graph_function,\n\u001B[0;32m   3038\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 3039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3040\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1959\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1961\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1962\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1963\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1964\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1965\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1966\u001B[0m     args,\n\u001B[0;32m   1967\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1968\u001B[0m     executing_eagerly)\n\u001B[0;32m   1969\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    590\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 591\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    597\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    598\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    599\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    600\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    603\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    604\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 59\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     62\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, TrainGenerator, ValidGenerator):\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            # 模型路径\n",
    "            filepath='./model/model_{epoch:02d}-{val_accuracy:.2f}.h5',\n",
    "            # 是否保存最佳\n",
    "            save_best_only=True,\n",
    "            # 监控指标\n",
    "            monitor='val_accuracy',\n",
    "            # 进度条类型\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(TrainGenerator,\n",
    "              steps_per_epoch=227,  #每批次总步数\n",
    "              epochs=10,  #批次数\n",
    "              validation_data=ValidGenerator,  #验证集\n",
    "              validation_steps=5,  #每次验证步数\n",
    "              validation_batch_size=64,  #验证集每批次数据大小\n",
    "              verbose=1,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "\n",
    "train_IR_path = '../data/Juliet/train_IR.txt'\n",
    "validation_IR_path = '../data/Juliet/validation_IR.txt'\n",
    "\n",
    "train_label_path = '../data/Juliet/train_label.txt'\n",
    "validation_label_path = '../data/Juliet/validation_label.txt'\n",
    "\n",
    "tdg = TrainDataGenerator(train_IR_path, train_label_path, batch_size=64)\n",
    "vdg = TrainDataGenerator(validation_IR_path, validation_label_path, batch_size=64)\n",
    "train_model(model, tdg, vdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "15\n",
      "TP:15 FP:4 FN:2 TN:43\n",
      "TP_l:15 FP_l:4 FN_l:2 TN:43\n",
      "epochs: 1\n",
      "TP:27 FP:4 FN:4 TN:93\n",
      "TP_l:27 FP_l:4 FN_l:4 TN:93\n",
      "epochs: 2\n",
      "TP:40 FP:4 FN:8 TN:140\n",
      "TP_l:40 FP_l:4 FN_l:8 TN:140\n",
      "epochs: 3\n",
      "TP:57 FP:5 FN:12 TN:182\n",
      "TP_l:57 FP_l:5 FN_l:12 TN:182\n",
      "TP:57 FP:5 FN:12 TN:182\n",
      "\n",
      "FPR: 0.026737967914438502\n",
      "\n",
      "FNR: 0.17391304347826086\n",
      "\n",
      "accuracy: 0.93359375\n",
      "\n",
      "precision: 0.9193548387096774\n",
      "\n",
      "recall: 0.8260869565217391\n",
      "\n",
      "F1_score: 0.8702290076335878\n",
      "\n",
      "\n",
      "TP_l:57 FP_l:5 FN_l:12 TN:182\n",
      "\n",
      "FPR_location: 0.026737967914438502\n",
      "\n",
      "FNR_location: 0.17391304347826086\n",
      "\n",
      "accuracy_location: 0.93359375\n",
      "\n",
      "precision_location: 0.9193548387096774\n",
      "\n",
      "recall_location: 0.8260869565217391\n",
      "\n",
      "loU: 0.11617313027226238\n",
      "\n",
      "loU: 0.11617313027226238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_predict_line(value_sequence, threshold_value=0.5):\n",
    "    value_sequence = list(value_sequence)\n",
    "    vs = len(value_sequence) - 1\n",
    "    while value_sequence[vs] == value_sequence[-1]:\n",
    "        vs -= 1\n",
    "    value_sequence = value_sequence[:vs + 2]\n",
    "\n",
    "    predict_line = []\n",
    "    for i in range(len(value_sequence)):\n",
    "        if value_sequence[i] > threshold_value:\n",
    "            predict_line.append(i)\n",
    "    return predict_line\n",
    "\n",
    "\n",
    "def test_model(model, datapath, labelpath, result_path):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    TP_l, TN_l, FP_l, FN_l = 0, 0, 0, 0\n",
    "    loU_list = []\n",
    "    partial_model = Model(inputs=model.layers[0].input, outputs=model.layers[7].output)\n",
    "    test_data = TestDataGenerator(datapath, labelpath, batch_size=64)\n",
    "    batch_size = 64\n",
    "    iter_num = 92\n",
    "    for i in range(4):\n",
    "        print(\"epochs: \" + str(i))\n",
    "        td = next(test_data)\n",
    "        output_test = partial_model([td[0][0]], training=False)\n",
    "        label = td[1]\n",
    "        vul_line = td[2]\n",
    "        # print(output_test.shape,label.shape,len(vul_line))\n",
    "        for j in range(batch_size):\n",
    "            predict_line = get_predict_line(output_test[j])\n",
    "            # print(output_test[i])\n",
    "            # print(predict_line)\n",
    "            # print(label)\n",
    "            if predict_line:\n",
    "                label_pred = 1\n",
    "            else:\n",
    "                label_pred = 0\n",
    "            # print(label_pred,label[j])\n",
    "            if label_pred == 0 and label[j] == 0:\n",
    "                TN += 1\n",
    "                TN_l += 1\n",
    "            if label_pred == 0 and label[j] == 1:\n",
    "                FN += 1\n",
    "                FN_l += 1\n",
    "            if label_pred == 1 and label[j] == 0:\n",
    "                FP += 1\n",
    "                FP_l += 1\n",
    "            if label_pred == 1 and label[j] == 1:\n",
    "                TP += 1\n",
    "                flag_l = False\n",
    "                for pred in predict_line:\n",
    "                    if pred in vul_line[j]:\n",
    "                        flag_l = True\n",
    "                        break\n",
    "                if flag_l:\n",
    "                    TP_l += 1\n",
    "                else:\n",
    "                    FN_l += 1\n",
    "                overlap_line = list(set(predict_line).intersection(set(vul_line[j])))\n",
    "                union_line = list(set(predict_line).union(set(vul_line[j])))\n",
    "                loU = len(overlap_line) / len(union_line)\n",
    "                loU_list.append(loU)\n",
    "        print('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN))\n",
    "        print('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l))\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    FPR_line = FP_l / (FP_l + TN_l)\n",
    "    FNR_line = FN_l / (TP_l + FN_l)\n",
    "    accuracy_line = (TP_l + TN_l) / (TP_l + FP_l + FN_l + TN_l)\n",
    "    precision_line = TP_l / (TP_l + FP_l)\n",
    "    recall_line = TP_l / (TP_l + FN_l)\n",
    "    F1_score_line = (2 * precision_line * recall_line) / (precision_line + recall_line)\n",
    "\n",
    "    loU = np.mean(loU_list)\n",
    "    # print('test_samples_num: ' + str(len(data)) + '\\n')\n",
    "    print('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN) + '\\n')\n",
    "    print('FPR: ' + str(FPR) + '\\n')\n",
    "    print('FNR: ' + str(FNR) + '\\n')\n",
    "    print('accuracy: ' + str(accuracy) + '\\n')\n",
    "    print('precision: ' + str(precision) + '\\n')\n",
    "    print('recall: ' + str(recall) + '\\n')\n",
    "    print('F1_score: ' + str(F1_score) + '\\n\\n')\n",
    "\n",
    "    print('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l) + '\\n')\n",
    "    print('FPR_location: ' + str(FPR_line) + '\\n')\n",
    "    print('FNR_location: ' + str(FNR_line) + '\\n')\n",
    "    print('accuracy_location: ' + str(accuracy_line) + '\\n')\n",
    "    print('precision_location: ' + str(precision_line) + '\\n')\n",
    "    print('recall_location: ' + str(recall_line) + '\\n')\n",
    "    print('F1_score_location: ' + str(F1_score_line) + '\\n\\n')\n",
    "\n",
    "    print('loU: ' + str(loU) + '\\n')\n",
    "\n",
    "    with open(result_path, 'a') as fwrite:\n",
    "        # fwrite.write('test_samples_num: ' + str(len(data)) + '\\n')\n",
    "        fwrite.write('TP:' + str(TP) + ' FP:' + str(FP) + ' FN:' + str(FN) + ' TN:' + str(TN) + '\\n')\n",
    "        fwrite.write('FPR: ' + str(FPR) + '\\n')\n",
    "        fwrite.write('FNR: ' + str(FNR) + '\\n')\n",
    "        fwrite.write('accuracy: ' + str(accuracy) + '\\n')\n",
    "        fwrite.write('precision: ' + str(precision) + '\\n')\n",
    "        fwrite.write('recall: ' + str(recall) + '\\n')\n",
    "        fwrite.write('F1_score: ' + str(F1_score) + '\\n\\n')\n",
    "\n",
    "        fwrite.write('TP_l:' + str(TP_l) + ' FP_l:' + str(FP_l) + ' FN_l:' + str(FN_l) + ' TN:' + str(TN_l) + '\\n')\n",
    "        fwrite.write('FPR_location: ' + str(FPR_line) + '\\n')\n",
    "        fwrite.write('FNR_location: ' + str(FNR_line) + '\\n')\n",
    "        fwrite.write('accuracy_location: ' + str(accuracy_line) + '\\n')\n",
    "        fwrite.write('precision_location: ' + str(precision_line) + '\\n')\n",
    "        fwrite.write('recall_location: ' + str(recall_line) + '\\n')\n",
    "        fwrite.write('F1_score_location: ' + str(F1_score_line) + '\\n\\n')\n",
    "\n",
    "        fwrite.write('loU: ' + str(loU) + '\\n')\n",
    "\n",
    "\n",
    "modelPath = './model/model_10-0.97.h5'\n",
    "resultPath = './result/result_model_10_0.97.txt'\n",
    "\n",
    "# test_IR_path = '../data/Juliet/test_IR.txt'\n",
    "# test_label_path = '../data/Juliet/test_label.txt'\n",
    "test_IR_path = '../data/Juliet/data_Juliet2.txt'\n",
    "test_label_path = '../data/Juliet/label_Juliet2.txt'\n",
    "\n",
    "model = models.load_model(modelPath)\n",
    "test_model(model, test_IR_path, test_label_path, resultPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## cs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型中间结果测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(64, 1000, 1) (64,) 64\n",
      "[0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_data = TestDataGenerator(test_IR_path, test_label_path, batch_size=64)\n",
    "td = next(test_data)\n",
    "partial_model = Model(inputs=model.layers[0].input, outputs=model.layers[7].output)\n",
    "output_test = partial_model([td[0][0]], training=False)\n",
    "label = td[1]\n",
    "vul_line = td[2]\n",
    "print(output_test.shape, label.shape, len(vul_line))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n",
      "[26.0, 27.0, 28.0]\n",
      "[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(vul_line[4])\n",
    "# print(output_test[4][0:50])\n",
    "predict_line = get_predict_line(output_test[4])\n",
    "print(predict_line)\n",
    "if predict_line:\n",
    "    label_pred = 1\n",
    "else:\n",
    "    label_pred = 0\n",
    "print(label_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型最终结果显示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m td \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[43mtest_data\u001B[49m)\n\u001B[0;32m      2\u001B[0m output\u001B[38;5;241m=\u001B[39mmodel([td[\u001B[38;5;241m0\u001B[39m]],training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m label\u001B[38;5;241m=\u001B[39mtd[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "td = next(test_data)\n",
    "output = model([td[0]], training=False)\n",
    "label = td[1]\n",
    "print(label)\n",
    "print(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "  9/100 [=>............................] - ETA: 4:04 - loss: 0.1923 - accuracy: 0.9306 - precision: 0.9638 - recall: 0.7917 - true_positives: 133.0000 - true_negatives: 403.0000 - false_positives: 5.0000 - false_negatives: 35.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m test_data2\u001B[38;5;241m=\u001B[39mDataGenerator(test_IR_path,test_label_path,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data2\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:1501\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1500\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[1;32m-> 1501\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1503\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    882\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 885\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    887\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    888\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    921\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    922\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 924\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateful_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[0;32m    926\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    927\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3036\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   3037\u001B[0m   (graph_function,\n\u001B[0;32m   3038\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 3039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3040\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1959\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1961\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1962\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1963\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1964\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1965\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1966\u001B[0m     args,\n\u001B[0;32m   1967\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1968\u001B[0m     executing_eagerly)\n\u001B[0;32m   1969\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    590\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 591\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    597\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    598\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    599\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    600\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    603\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    604\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\program_files\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 59\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     62\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_data2 = TrainDataGenerator(test_IR_path, test_label_path, batch_size=64)\n",
    "model.evaluate(test_data2, batch_size=64, steps=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dab8e41921a9804032d7b1bd163ac623c44de801f9b5a9764714bd0c64231e68"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}